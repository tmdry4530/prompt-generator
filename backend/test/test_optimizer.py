#!/usr/bin/env python3
"""
Optimizer ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_optimizer():
    """Optimizer ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        print("ğŸ”§ Optimizer ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # 1. ê¸°ë³¸ import í…ŒìŠ¤íŠ¸
        from src.services.optimizer import PromptOptimizer
        print("âœ… PromptOptimizer import ì„±ê³µ")
        
        # 2. ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
        optimizer = PromptOptimizer()
        print("âœ… PromptOptimizer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ")
        
        # 3. ëª¨ë¸ ë¡œë“œ í™•ì¸
        print(f"ğŸ“Š ë¡œë“œëœ ëª¨ë¸ ìˆ˜: {len(optimizer.models)}")
        print("ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
        for model_id in sorted(optimizer.models.keys()):
            model = optimizer.models[model_id]
            print(f"  - {model_id}: {model.model_name} (ì œê³µì—…ì²´: {model.provider})")
        
        # 4. ëª¨ë¸ë³„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        if optimizer.models:
            test_model_id = list(optimizer.models.keys())[0]
            print(f"\nğŸ§ª '{test_model_id}' ëª¨ë¸ë¡œ ìµœì í™” í…ŒìŠ¤íŠ¸...")
            
            test_input = "ë°ê³  í™”ì°½í•œ ë‚ ì— í•´ë³€ì—ì„œ ë›°ë…¸ëŠ” ê°•ì•„ì§€ì˜ ì‚¬ì§„ì„ ìƒì„±í•´ì£¼ì„¸ìš”"
            result = optimizer.optimize_prompt(test_input, test_model_id)
            
            if result.get("success"):
                print("âœ… í”„ë¡¬í”„íŠ¸ ìµœì í™” ì„±ê³µ!")
                print(f"ğŸ“ ì›ë³¸ ì…ë ¥: {result['original_input']}")
                print(f"âš¡ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸: {result['optimized_prompt'][:200]}...")
                
                if result.get("generation_params"):
                    print(f"âš™ï¸ ìƒì„± ë§¤ê°œë³€ìˆ˜: {list(result['generation_params'].keys())}")
            else:
                print(f"âŒ í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹¤íŒ¨: {result.get('error')}")
        
        print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_optimizer()

"""
PromptOptimizer í´ë˜ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸
"""

import pytest
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, Any, List
from src.services.optimizer import PromptOptimizer


class TestPromptOptimizer:
    """PromptOptimizer í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def optimizer(self):
        """PromptOptimizer ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return PromptOptimizer()
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    def test_optimizer_initialization(self, optimizer):
        """Optimizer ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        assert optimizer is not None
        assert hasattr(optimizer, 'input_analyzer')
        assert hasattr(optimizer, 'models')
        
        # ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert len(optimizer.models) > 0
        
        # ê° ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸
        for model_id, model in optimizer.models.items():
            assert hasattr(model, 'model_id')
            assert hasattr(model, 'model_name')
            assert hasattr(model, 'provider')
            assert hasattr(model, 'capabilities')
            assert hasattr(model, 'optimize_prompt')
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    def test_get_available_models(self, optimizer):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        models = optimizer.get_available_models()
        
        # ëª¨ë¸ ëª©ë¡ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
        assert len(models) > 0
        
        # ê° ëª¨ë¸ ì •ë³´ê°€ ì˜¬ë°”ë¥¸ êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸
        for model_info in models:
            required_keys = ["model_id", "model_name", "provider", "capabilities", "supports_multimodal"]
            for key in required_keys:
                assert key in model_info, f"Required key '{key}' is missing from model info"
            
            assert isinstance(model_info["capabilities"], list)
            assert isinstance(model_info["supports_multimodal"], bool)
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    @pytest.mark.parametrize("model_id,input_text", [
        ("gpt-4o", "ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."),
        ("imagen-3", "ë°ê³  í™”ì°½í•œ ë‚ ì— í•´ë³€ì—ì„œ ë›°ë…¸ëŠ” ê°•ì•„ì§€ì˜ ì‚¬ì§„"),
        ("suno", "ë°ê³  ê²½ì¾Œí•œ íŒ ìŒì•…ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”"),
        ("sora", "ë„ì‹œë¥¼ ë°°ê²½ìœ¼ë¡œ í•œ ì§§ì€ ë¹„ë””ì˜¤ í´ë¦½")
    ])
    def test_optimize_prompt_success(self, optimizer, model_id, input_text):
        """í”„ë¡¬í”„íŠ¸ ìµœì í™” ì„±ê³µ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
        result = optimizer.optimize_prompt(input_text, model_id)
        
        # ì„±ê³µì ì¸ ê²°ê³¼ì¸ì§€ í™•ì¸
        assert result["success"] is True
        assert "optimized_prompt" in result
        assert "model_info" in result
        assert "prompt_structure" in result
        
        # ì…ë ¥ê³¼ ëª¨ë¸ IDê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert result["original_input"] == input_text
        assert result["model_id"] == model_id
        
        # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
        assert len(result["optimized_prompt"]) > 0
        assert result["optimized_prompt"] != input_text  # ìµœì í™”ê°€ ì‹¤ì œë¡œ ì¼ì–´ë‚¬ëŠ”ì§€ í™•ì¸
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    def test_optimize_prompt_with_additional_params(self, optimizer):
        """ì¶”ê°€ ë§¤ê°œë³€ìˆ˜ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        input_text = "ë³µì¡í•œ ê¸°ìˆ  ë¬¸ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”"
        model_id = "gpt-4o"
        additional_params = {
            "complexity": "high",
            "style": [("technical", 0.9)],
            "constraints": {"tone": "professional", "audience": "expert"}
        }
        
        result = optimizer.optimize_prompt(input_text, model_id, additional_params)
        
        assert result["success"] is True
        
        # ì¶”ê°€ ë§¤ê°œë³€ìˆ˜ê°€ ë¶„ì„ ê²°ê³¼ì— í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
        analysis_result = result["analysis_result"]
        assert analysis_result["complexity"] == "high"
        assert analysis_result["style"] == [("technical", 0.9)]
        assert analysis_result["constraints"]["tone"] == "professional"
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    def test_optimize_prompt_invalid_model(self, optimizer):
        """ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ë¡œ ìµœì í™” ìš”ì²­ í…ŒìŠ¤íŠ¸"""
        input_text = "í…ŒìŠ¤íŠ¸ ì…ë ¥"
        invalid_model_id = "non-existent-model"
        
        result = optimizer.optimize_prompt(input_text, invalid_model_id)
        
        # ì‹¤íŒ¨ ê²°ê³¼ì¸ì§€ í™•ì¸
        assert result["success"] is False
        assert "error" in result
        assert "available_models" in result
        assert invalid_model_id not in result["available_models"]
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    def test_get_model_specific_tips(self, optimizer):
        """ëª¨ë¸ë³„ ìµœì í™” íŒ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ
        available_models = list(optimizer.models.keys())
        test_model_id = available_models[0]
        
        tips = optimizer.get_model_specific_tips(test_model_id)
        
        # íŒì´ ë°˜í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert isinstance(tips, list)
        assert len(tips) > 0
        
        # íŒì´ ë¬¸ìì—´ì¸ì§€ í™•ì¸
        for tip in tips:
            assert isinstance(tip, str)
            assert len(tip) > 0
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    def test_get_model_prompt_structure(self, optimizer):
        """ëª¨ë¸ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ
        available_models = list(optimizer.models.keys())
        test_model_id = available_models[0]
        
        structure = optimizer.get_model_prompt_structure(test_model_id)
        
        # êµ¬ì¡° ì •ë³´ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
        assert isinstance(structure, dict)
        assert "components" in structure
        assert "recommended_order" in structure
        
        # ì»´í¬ë„ŒíŠ¸ê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
        assert isinstance(structure["components"], list)
        assert isinstance(structure["recommended_order"], list)
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    def test_get_model_info(self, optimizer):
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ
        available_models = list(optimizer.models.keys())
        test_model_id = available_models[0]
        
        model_info = optimizer.get_model_info(test_model_id)
        
        # ëª¨ë¸ ì •ë³´ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
        assert isinstance(model_info, dict)
        required_keys = ["model_id", "model_name", "provider", "capabilities", "supports_multimodal"]
        for key in required_keys:
            assert key in model_info, f"Required key '{key}' is missing from model info"
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    def test_compare_models(self, optimizer):
        """ëª¨ë¸ ë¹„êµ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ ì¼ë¶€ë¥¼ ì„ íƒ
        available_models = list(optimizer.models.keys())
        test_model_ids = available_models[:min(3, len(available_models))]
        
        comparison = optimizer.compare_models(test_model_ids)
        
        # ë¹„êµ ê²°ê³¼ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
        assert isinstance(comparison, dict)
        assert "models" in comparison
        assert "capabilities" in comparison
        assert "providers" in comparison
        assert "multimodal_support" in comparison
        
        # ìš”ì²­í•œ ëª¨ë¸ ìˆ˜ë§Œí¼ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
        assert len(comparison["models"]) == len(test_model_ids)
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    @pytest.mark.parametrize("edge_case", [
        {"name": "ë¹ˆ ì…ë ¥", "input": "", "model_id": "gpt-4o"},
        {"name": "ë§¤ìš° ê¸´ ì…ë ¥", "input": "í…ŒìŠ¤íŠ¸ " * 1000, "model_id": "gpt-4o"},
        {"name": "íŠ¹ìˆ˜ ë¬¸ì", "input": "!@#$%^&*()", "model_id": "gpt-4o"},
        {"name": "ë‹¤êµ­ì–´", "input": "Hello ì•ˆë…•í•˜ì„¸ìš” ã“ã‚“ã«ã¡ã¯", "model_id": "gpt-4o"}
    ])
    def test_edge_cases(self, optimizer, edge_case):
        """ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
        try:
            result = optimizer.optimize_prompt(edge_case["input"], edge_case["model_id"])
            
            # ê²°ê³¼ê°€ ë°˜í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
            assert isinstance(result, dict)
            assert "success" in result
            
            # ë¹ˆ ì…ë ¥ì´ ì•„ë‹Œ ê²½ìš° ì„±ê³µí•´ì•¼ í•¨
            if edge_case["input"].strip():
                assert result["success"] is True
                assert "optimized_prompt" in result
            
        except Exception as e:
            pytest.fail(f"Edge case '{edge_case['name']}' failed with error: {str(e)}")
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    @pytest.mark.slow
    def test_performance_multiple_requests(self, optimizer):
        """ë‹¤ì¤‘ ìš”ì²­ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        import time
        
        input_text = "ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì…ë ¥ í…ìŠ¤íŠ¸"
        model_id = "gpt-4o"
        num_requests = 10
        
        start_time = time.time()
        
        results = []
        for _ in range(num_requests):
            result = optimizer.optimize_prompt(input_text, model_id)
            results.append(result)
        
        end_time = time.time()
        total_time = end_time - start_time
        avg_time = total_time / num_requests
        
        # ëª¨ë“  ìš”ì²­ì´ ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸
        assert all(result["success"] for result in results)
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„ì´ í•©ë¦¬ì ì¸ì§€ í™•ì¸ (2ì´ˆ ì´í•˜)
        assert avg_time < 2.0, f"Average response time too high: {avg_time:.2f}s"
        
        print(f"Average response time: {avg_time:.3f}s for {num_requests} requests")
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    def test_model_specific_generation_parameters(self, optimizer):
        """ëª¨ë¸ë³„ ìƒì„± ë§¤ê°œë³€ìˆ˜ í…ŒìŠ¤íŠ¸"""
        test_cases = [
            {
                "model_id": "imagen-3",
                "input": "ê³ í’ˆì§ˆ ì´ˆìƒí™” ì‚¬ì§„",
                "expected_params": ["width", "height", "quality"]
            },
            {
                "model_id": "sora",
                "input": "ì§§ì€ ë™ì˜ìƒ í´ë¦½",
                "expected_params": ["duration", "resolution", "fps"]
            },
            {
                "model_id": "suno",
                "input": "íŒ ìŒì•…",
                "expected_params": ["duration", "tempo", "quality"]
            }
        ]
        
        for case in test_cases:
            if case["model_id"] in optimizer.models:
                result = optimizer.optimize_prompt(case["input"], case["model_id"])
                
                if result["success"] and "generation_params" in result:
                    gen_params = result["generation_params"]
                    
                    # ì˜ˆìƒ ë§¤ê°œë³€ìˆ˜ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    for param in case["expected_params"]:
                        assert param in gen_params, f"Expected parameter '{param}' not found in generation params"
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    def test_cross_model_consistency(self, optimizer):
        """ëª¨ë¸ ê°„ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸"""
        input_text = "ì°½ì˜ì ì¸ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”"
        
        results = {}
        for model_id in optimizer.models.keys():
            result = optimizer.optimize_prompt(input_text, model_id)
            results[model_id] = result
        
        # ëª¨ë“  ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í–ˆëŠ”ì§€ í™•ì¸
        for model_id, result in results.items():
            assert result["success"] is True, f"Model {model_id} failed to process input"
            assert "optimized_prompt" in result
            assert result["original_input"] == input_text
            assert result["model_id"] == model_id
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    def test_error_handling_and_recovery(self, optimizer):
        """ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬ í…ŒìŠ¤íŠ¸"""
        # ëª¨ì˜ ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ê¸°ì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œë®¬ë ˆì´ì…˜
        with patch.object(optimizer.input_analyzer, 'analyze') as mock_analyze:
            mock_analyze.side_effect = Exception("Simulated analysis error")
            
            result = optimizer.optimize_prompt("í…ŒìŠ¤íŠ¸ ì…ë ¥", "gpt-4o")
            
            # ì˜¤ë¥˜ê°€ ì ì ˆíˆ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
            assert result["success"] is False
            assert "error" in result
            assert "Simulated analysis error" in result["error"]
    
    @pytest.mark.integration
    @pytest.mark.optimizer
    @pytest.mark.benchmark
    def test_optimization_benchmark(self, optimizer, benchmark):
        """ìµœì í™” ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        input_text = "ì¤‘ê°„ í¬ê¸°ì˜ í…ìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤. " * 20
        model_id = "gpt-4o"
        
        # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        result = benchmark(optimizer.optimize_prompt, input_text, model_id)
        
        # ê²°ê³¼ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
        assert result["success"] is True
        assert "optimized_prompt" in result


class TestPromptOptimizerMocked:
    """ëª¨ì˜ ê°ì²´ë¥¼ ì‚¬ìš©í•œ PromptOptimizer í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def mock_optimizer(self, mock_input_analyzer, mock_base_model):
        """ëª¨ì˜ ê°ì²´ë¥¼ ì‚¬ìš©í•œ PromptOptimizerë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        with patch('src.services.optimizer.InputAnalyzer', return_value=mock_input_analyzer):
            optimizer = PromptOptimizer()
            # ëª¨ì˜ ëª¨ë¸ ì¶”ê°€
            optimizer.models = {"test-model": mock_base_model}
            return optimizer
    
    @pytest.mark.unit
    @pytest.mark.optimizer
    def test_optimize_prompt_with_mocks(self, mock_optimizer, mock_input_analyzer, mock_base_model):
        """ëª¨ì˜ ê°ì²´ë¥¼ ì‚¬ìš©í•œ í”„ë¡¬í”„íŠ¸ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        input_text = "í…ŒìŠ¤íŠ¸ ì…ë ¥"
        model_id = "test-model"
        
        result = mock_optimizer.optimize_prompt(input_text, model_id)
        
        # ëª¨ì˜ ê°ì²´ ë©”ì„œë“œê°€ í˜¸ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸
        mock_input_analyzer.analyze.assert_called_once_with(input_text, model_id)
        mock_base_model.optimize_prompt.assert_called_once()
        
        # ê²°ê³¼ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
        assert result["success"] is True
        assert result["optimized_prompt"] == "ìµœì í™”ëœ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸"
    
    @pytest.mark.unit
    @pytest.mark.optimizer
    def test_get_available_models_with_mocks(self, mock_optimizer):
        """ëª¨ì˜ ê°ì²´ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        models = mock_optimizer.get_available_models()
        
        assert len(models) == 1
        assert models[0]["model_id"] == "test-model"
        assert models[0]["model_name"] == "Test Model"
    
    @pytest.mark.unit
    @pytest.mark.optimizer
    def test_error_handling_with_mocks(self, mock_optimizer, mock_input_analyzer):
        """ëª¨ì˜ ê°ì²´ë¥¼ ì‚¬ìš©í•œ ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # ë¶„ì„ê¸°ì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œë®¬ë ˆì´ì…˜
        mock_input_analyzer.analyze.side_effect = Exception("Mock error")
        
        result = mock_optimizer.optimize_prompt("í…ŒìŠ¤íŠ¸", "test-model")
        
        assert result["success"] is False
        assert "Mock error" in result["error"] 